{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediccion de Diabetes utilizando un perceptron simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un perceptrón en IA es la unidad básica de una red neuronal. Con Keras, es un modelo de red neuronal de una sola capa con una función de activación. Toma entradas ponderadas, las suma y aplica una función de activación para producir una salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un perceptrón consta de entradas (features), pesos (synaptic weights), una suma ponderada, un sesgo (bias), una función de activación y una salida. Cada entrada se multiplica por su peso correspondiente, se suman, se agrega el sesgo y se pasa a través de la función de activación. La función de activación introduce no linealidades, permitiendo que el perceptrón aprenda patrones más complejos. En términos de teoría, los perceptrones son bloques fundamentales de las redes neuronales, y al agrupar múltiples perceptrones, se pueden modelar relaciones más sofisticadas y resolver problemas complejos. La capacidad de ajustar pesos durante el entrenamiento permite al perceptrón aprender de los datos y mejorar su rendimiento en tareas específicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importamos las librerias que vamos a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QAqbriyvElAk"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comentamos esta linea de código debido a que solo se utiliza si estamos en un entorno como Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "onuGgNlqFguR",
        "outputId": "6be9c353-2114-4fd2-ab18-14ae0fb2334b"
      },
      "outputs": [],
      "source": [
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En caso de trabajar en un entorno local utilizamos esta linea que importa el dataset desde la raíz de la carpeta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HtrI-tSZE1Tn"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"diabetes-2.csv\", encoding=\"utf-8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizamos los datos de la tabla en forma de media, desviacion estandar, minimos, cuartiles y maximo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "oT-lla6wHjaj",
        "outputId": "50e62cd9-d423-4ee0-f642-907197f71a09"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
              "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
              "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
              "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
              "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
              "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
              "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
              "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
              "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
              "\n",
              "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
              "count  768.000000                768.000000  768.000000  768.000000  \n",
              "mean    31.992578                  0.471876   33.240885    0.348958  \n",
              "std      7.884160                  0.331329   11.760232    0.476951  \n",
              "min      0.000000                  0.078000   21.000000    0.000000  \n",
              "25%     27.300000                  0.243750   24.000000    0.000000  \n",
              "50%     32.000000                  0.372500   29.000000    0.000000  \n",
              "75%     36.600000                  0.626250   41.000000    1.000000  \n",
              "max     67.100000                  2.420000   81.000000    1.000000  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seleccionamos las columnas que vamos a utilziar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNc_dmneHGfQ",
        "outputId": "17bc11ea-33c0-4721-f667-f3bf8dc9f1a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Campos Glucose: con entradas en 0: 5\n",
            "Campos BloodPressure: con entradas en 0: 35\n",
            "Campos SkinThickness: con entradas en 0: 227\n",
            "Campos Insulin: con entradas en 0: 374\n",
            "Campos BMI: con entradas en 0: 11\n"
          ]
        }
      ],
      "source": [
        "valores_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "def revisar_los_valores_zero(data, fields):\n",
        "\n",
        "    for field in fields:\n",
        "        print('Campos %s: con entradas en 0: %d' % (field, len(data.loc[ data[field] == 0, field ])))\n",
        "\n",
        "revisar_los_valores_zero(dataset, valores_zero)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizamos un conjunto de datos con características ('features') y la variable objetivo 'Outcome'. Se divide el conjunto en entrenamiento y prueba (75% entrenamiento, 25% prueba) usando `train_test_split` de scikit-learn. Luego, se imprime la longitud de los conjuntos de entrenamiento y prueba. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykbmjw4eH_vl",
        "outputId": "4dd6b87a-a4ec-4f86-d6fb-9570438d008a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
            "Longitud de entrenamiento (576, 8)\n",
            "Longitud de prueba (192, 8)\n"
          ]
        }
      ],
      "source": [
        "features = list(dataset.columns.values)\n",
        "features.remove('Outcome')\n",
        "print(features)\n",
        "X = dataset[features]\n",
        "y = dataset['Outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"Longitud de entrenamiento\" , X_train.shape)\n",
        "print(\"Longitud de prueba\" , X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8uTUI0PIbci"
      },
      "source": [
        "Calculamos el promedio de los valores no cero en ese campo y lo utiliza para reemplazar los ceros. Luego, imprime un mensaje indicando el campo, la cantidad de entradas actualizadas y el nuevo valor promedio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PMesJkE2IYML"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En el campo: Glucose;  fueron actualizadas 4 entradas con el valor: 122.003\n",
            "En el campo: BloodPressure;  fueron actualizadas 25 entradas con el valor: 72.846\n",
            "En el campo: SkinThickness;  fueron actualizadas 163 entradas con el valor: 29.465\n",
            "En el campo: Insulin;  fueron actualizadas 270 entradas con el valor: 158.464\n",
            "En el campo: BMI;  fueron actualizadas 8 entradas con el valor: 32.663\n"
          ]
        }
      ],
      "source": [
        "def rellenar_valores_zeros(data, field):\n",
        "    nonzero_vals = data.loc[data[field] != 0, field]\n",
        "    avg = np.sum(nonzero_vals) / len(nonzero_vals)\n",
        "    k = len(data.loc[ data[field] == 0, field])\n",
        "    data.loc[ data[field] == 0, field ] = avg\n",
        "    print('En el campo: %s;  fueron actualizadas %d entradas con el valor: %.3f' % (field, k, avg))\n",
        "\n",
        "for campo in valores_zero:\n",
        "    rellenar_valores_zeros(X_train, campo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5PoKH5TJiDl"
      },
      "source": [
        "Dejamos solo los valores, y no los nombres de la columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ivMjR-FaJfer"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.values\n",
        "y_train = y_train.values\n",
        "X_test  = X_test.values\n",
        "y_test  = y_test.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La primera capa tiene 32 nodos con activación ReLU y 8 dimensiones de entrada. La segunda capa tiene 16 nodos con activación ReLU. La capa de salida tiene 1 nodo con activación sigmoide para problemas de clasificación binaria. Utiliza el optimizador 'adam' y la función de pérdida 'binary_crossentropy'. Entrena el modelo con los datos de entrenamiento (X_train y y_train) durante 250 épocas.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeyK4tT-MAUM",
        "outputId": "fa4dd025-c02c-4129-b696-4cd474e3ce82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 1ms/step - loss: 2.4373 - accuracy: 0.5573\n",
            "Epoch 2/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 2.0886 - accuracy: 0.5764\n",
            "Epoch 3/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 1.5155 - accuracy: 0.5972\n",
            "Epoch 4/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 1.1574 - accuracy: 0.5955\n",
            "Epoch 5/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 1.0933 - accuracy: 0.6076\n",
            "Epoch 6/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 1.0003 - accuracy: 0.6302\n",
            "Epoch 7/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.9335 - accuracy: 0.6319\n",
            "Epoch 8/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.7781 - accuracy: 0.6441\n",
            "Epoch 9/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.7222 - accuracy: 0.6458\n",
            "Epoch 10/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.7633 - accuracy: 0.6424\n",
            "Epoch 11/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.7032 - accuracy: 0.6493\n",
            "Epoch 12/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6958 - accuracy: 0.6493\n",
            "Epoch 13/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6706 - accuracy: 0.6806\n",
            "Epoch 14/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.7426 - accuracy: 0.6684\n",
            "Epoch 15/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6981 - accuracy: 0.6528\n",
            "Epoch 16/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6545 - accuracy: 0.6736\n",
            "Epoch 17/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.6295 - accuracy: 0.6771\n",
            "Epoch 18/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6214 - accuracy: 0.6771\n",
            "Epoch 19/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6017 - accuracy: 0.6840\n",
            "Epoch 20/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6104 - accuracy: 0.7083\n",
            "Epoch 21/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6154 - accuracy: 0.6875\n",
            "Epoch 22/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6144 - accuracy: 0.6823\n",
            "Epoch 23/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5825 - accuracy: 0.6962\n",
            "Epoch 24/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5854 - accuracy: 0.7083\n",
            "Epoch 25/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6098 - accuracy: 0.6962\n",
            "Epoch 26/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5793 - accuracy: 0.6979\n",
            "Epoch 27/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6434 - accuracy: 0.6840\n",
            "Epoch 28/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6580 - accuracy: 0.6701\n",
            "Epoch 29/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5824 - accuracy: 0.7240\n",
            "Epoch 30/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5622 - accuracy: 0.7101\n",
            "Epoch 31/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5646 - accuracy: 0.7170\n",
            "Epoch 32/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5682 - accuracy: 0.6892\n",
            "Epoch 33/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6089 - accuracy: 0.6979\n",
            "Epoch 34/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6223 - accuracy: 0.6962\n",
            "Epoch 35/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5950 - accuracy: 0.6944\n",
            "Epoch 36/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5639 - accuracy: 0.7188\n",
            "Epoch 37/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6128 - accuracy: 0.7014\n",
            "Epoch 38/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6078 - accuracy: 0.6997\n",
            "Epoch 39/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5564 - accuracy: 0.7049\n",
            "Epoch 40/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5479 - accuracy: 0.7170\n",
            "Epoch 41/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5604 - accuracy: 0.7222\n",
            "Epoch 42/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5482 - accuracy: 0.7188\n",
            "Epoch 43/250\n",
            "18/18 [==============================] - 0s 941us/step - loss: 0.5628 - accuracy: 0.7188\n",
            "Epoch 44/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5964 - accuracy: 0.7118\n",
            "Epoch 45/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5687 - accuracy: 0.7083\n",
            "Epoch 46/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5633 - accuracy: 0.7188\n",
            "Epoch 47/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5616 - accuracy: 0.7118\n",
            "Epoch 48/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5648 - accuracy: 0.7049\n",
            "Epoch 49/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5680 - accuracy: 0.7170\n",
            "Epoch 50/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5552 - accuracy: 0.7257\n",
            "Epoch 51/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5705 - accuracy: 0.7135\n",
            "Epoch 52/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5700 - accuracy: 0.6962\n",
            "Epoch 53/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5797 - accuracy: 0.7135\n",
            "Epoch 54/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5613 - accuracy: 0.7188\n",
            "Epoch 55/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6641 - accuracy: 0.6875\n",
            "Epoch 56/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6311 - accuracy: 0.6997\n",
            "Epoch 57/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5791 - accuracy: 0.7188\n",
            "Epoch 58/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5511 - accuracy: 0.7188\n",
            "Epoch 59/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5449 - accuracy: 0.7257\n",
            "Epoch 60/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5507 - accuracy: 0.7309\n",
            "Epoch 61/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5627 - accuracy: 0.7292\n",
            "Epoch 62/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.6239 - accuracy: 0.6823\n",
            "Epoch 63/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5509 - accuracy: 0.7170\n",
            "Epoch 64/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5954 - accuracy: 0.6927\n",
            "Epoch 65/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5727 - accuracy: 0.7240\n",
            "Epoch 66/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5688 - accuracy: 0.7344\n",
            "Epoch 67/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5708 - accuracy: 0.7066\n",
            "Epoch 68/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5513 - accuracy: 0.7205\n",
            "Epoch 69/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5569 - accuracy: 0.7222\n",
            "Epoch 70/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5617 - accuracy: 0.7153\n",
            "Epoch 71/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5450 - accuracy: 0.7378\n",
            "Epoch 72/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5439 - accuracy: 0.7309\n",
            "Epoch 73/250\n",
            "18/18 [==============================] - 0s 1000us/step - loss: 0.5776 - accuracy: 0.7222\n",
            "Epoch 74/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5684 - accuracy: 0.7344\n",
            "Epoch 75/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5320 - accuracy: 0.7344\n",
            "Epoch 76/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5323 - accuracy: 0.7413\n",
            "Epoch 77/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5379 - accuracy: 0.7222\n",
            "Epoch 78/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5442 - accuracy: 0.7066\n",
            "Epoch 79/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5344 - accuracy: 0.7292\n",
            "Epoch 80/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5346 - accuracy: 0.7292\n",
            "Epoch 81/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5781 - accuracy: 0.7014\n",
            "Epoch 82/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.7167 - accuracy: 0.6806\n",
            "Epoch 83/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5800 - accuracy: 0.7135\n",
            "Epoch 84/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5303 - accuracy: 0.7413\n",
            "Epoch 85/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5533 - accuracy: 0.7257\n",
            "Epoch 86/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.6136 - accuracy: 0.6892\n",
            "Epoch 87/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5936 - accuracy: 0.7118\n",
            "Epoch 88/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5379 - accuracy: 0.7274\n",
            "Epoch 89/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5381 - accuracy: 0.7049\n",
            "Epoch 90/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5355 - accuracy: 0.7205\n",
            "Epoch 91/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5435 - accuracy: 0.7309\n",
            "Epoch 92/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5169 - accuracy: 0.7517\n",
            "Epoch 93/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5254 - accuracy: 0.7622\n",
            "Epoch 94/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5270 - accuracy: 0.7309\n",
            "Epoch 95/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5419 - accuracy: 0.7274\n",
            "Epoch 96/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5415 - accuracy: 0.7083\n",
            "Epoch 97/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5419 - accuracy: 0.7413\n",
            "Epoch 98/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5142 - accuracy: 0.7413\n",
            "Epoch 99/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5522 - accuracy: 0.7135\n",
            "Epoch 100/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5281 - accuracy: 0.7309\n",
            "Epoch 101/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5689 - accuracy: 0.7031\n",
            "Epoch 102/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5831 - accuracy: 0.7031\n",
            "Epoch 103/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5158 - accuracy: 0.7500\n",
            "Epoch 104/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5351 - accuracy: 0.7205\n",
            "Epoch 105/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7587\n",
            "Epoch 106/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5321 - accuracy: 0.7205\n",
            "Epoch 107/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5232 - accuracy: 0.7292\n",
            "Epoch 108/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5133 - accuracy: 0.7344\n",
            "Epoch 109/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5147 - accuracy: 0.7274\n",
            "Epoch 110/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5211 - accuracy: 0.7396\n",
            "Epoch 111/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5763 - accuracy: 0.7240\n",
            "Epoch 112/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5358 - accuracy: 0.7222\n",
            "Epoch 113/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5121 - accuracy: 0.7483\n",
            "Epoch 114/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5400 - accuracy: 0.7431\n",
            "Epoch 115/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5426 - accuracy: 0.7326\n",
            "Epoch 116/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5483 - accuracy: 0.7188\n",
            "Epoch 117/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5904 - accuracy: 0.7083\n",
            "Epoch 118/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5700 - accuracy: 0.7135\n",
            "Epoch 119/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5700 - accuracy: 0.7031\n",
            "Epoch 120/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5195 - accuracy: 0.7569\n",
            "Epoch 121/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5402 - accuracy: 0.7240\n",
            "Epoch 122/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5302 - accuracy: 0.7153\n",
            "Epoch 123/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5517 - accuracy: 0.7378\n",
            "Epoch 124/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5368 - accuracy: 0.7413\n",
            "Epoch 125/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5288 - accuracy: 0.7274\n",
            "Epoch 126/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5151 - accuracy: 0.7413\n",
            "Epoch 127/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.7552\n",
            "Epoch 128/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5011 - accuracy: 0.7500\n",
            "Epoch 129/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5281 - accuracy: 0.7361\n",
            "Epoch 130/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5646 - accuracy: 0.6979\n",
            "Epoch 131/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5685 - accuracy: 0.7431\n",
            "Epoch 132/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5447 - accuracy: 0.7101\n",
            "Epoch 133/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5230 - accuracy: 0.7188\n",
            "Epoch 134/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5694 - accuracy: 0.7066\n",
            "Epoch 135/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5310 - accuracy: 0.7205\n",
            "Epoch 136/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5032 - accuracy: 0.7431\n",
            "Epoch 137/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5124 - accuracy: 0.7431\n",
            "Epoch 138/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5307 - accuracy: 0.7309\n",
            "Epoch 139/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5351 - accuracy: 0.7222\n",
            "Epoch 140/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6103 - accuracy: 0.6875\n",
            "Epoch 141/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5285 - accuracy: 0.7448\n",
            "Epoch 142/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5269 - accuracy: 0.7431\n",
            "Epoch 143/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5609 - accuracy: 0.7153\n",
            "Epoch 144/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5446 - accuracy: 0.7188\n",
            "Epoch 145/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5681 - accuracy: 0.7101\n",
            "Epoch 146/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5034 - accuracy: 0.7431\n",
            "Epoch 147/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5539 - accuracy: 0.7135\n",
            "Epoch 148/250\n",
            "18/18 [==============================] - 0s 764us/step - loss: 0.5086 - accuracy: 0.7465\n",
            "Epoch 149/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5083 - accuracy: 0.7604\n",
            "Epoch 150/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5138 - accuracy: 0.7483\n",
            "Epoch 151/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.6251 - accuracy: 0.6979\n",
            "Epoch 152/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5441 - accuracy: 0.7205\n",
            "Epoch 153/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5243 - accuracy: 0.7378\n",
            "Epoch 154/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.4974 - accuracy: 0.7465\n",
            "Epoch 155/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5035 - accuracy: 0.7344\n",
            "Epoch 156/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5057 - accuracy: 0.7500\n",
            "Epoch 157/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5137 - accuracy: 0.7309\n",
            "Epoch 158/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5099 - accuracy: 0.7500\n",
            "Epoch 159/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5051 - accuracy: 0.7517\n",
            "Epoch 160/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5191 - accuracy: 0.7465\n",
            "Epoch 161/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5129 - accuracy: 0.7500\n",
            "Epoch 162/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5158 - accuracy: 0.7361\n",
            "Epoch 163/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5244 - accuracy: 0.7205\n",
            "Epoch 164/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5056 - accuracy: 0.7535\n",
            "Epoch 165/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5054 - accuracy: 0.7517\n",
            "Epoch 166/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5108 - accuracy: 0.7500\n",
            "Epoch 167/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5156 - accuracy: 0.7378\n",
            "Epoch 168/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5077 - accuracy: 0.7622\n",
            "Epoch 169/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5028 - accuracy: 0.7500\n",
            "Epoch 170/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4991 - accuracy: 0.7500\n",
            "Epoch 171/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4985 - accuracy: 0.7483\n",
            "Epoch 172/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5002 - accuracy: 0.7656\n",
            "Epoch 173/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5051 - accuracy: 0.7552\n",
            "Epoch 174/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5246 - accuracy: 0.7517\n",
            "Epoch 175/250\n",
            "18/18 [==============================] - 0s 1000us/step - loss: 0.5124 - accuracy: 0.7413\n",
            "Epoch 176/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5017 - accuracy: 0.7535\n",
            "Epoch 177/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5001 - accuracy: 0.7569\n",
            "Epoch 178/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5304 - accuracy: 0.7326\n",
            "Epoch 179/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4996 - accuracy: 0.7465\n",
            "Epoch 180/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4996 - accuracy: 0.7622\n",
            "Epoch 181/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5367 - accuracy: 0.7326\n",
            "Epoch 182/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.6962 - accuracy: 0.6806\n",
            "Epoch 183/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.6008 - accuracy: 0.7153\n",
            "Epoch 184/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5192 - accuracy: 0.7344\n",
            "Epoch 185/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5054 - accuracy: 0.7569\n",
            "Epoch 186/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5226 - accuracy: 0.7274\n",
            "Epoch 187/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4982 - accuracy: 0.7500\n",
            "Epoch 188/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5129 - accuracy: 0.7292\n",
            "Epoch 189/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5033 - accuracy: 0.7569\n",
            "Epoch 190/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5130 - accuracy: 0.7483\n",
            "Epoch 191/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5024 - accuracy: 0.7431\n",
            "Epoch 192/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5065 - accuracy: 0.7465\n",
            "Epoch 193/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5191 - accuracy: 0.7465\n",
            "Epoch 194/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5044 - accuracy: 0.7396\n",
            "Epoch 195/250\n",
            "18/18 [==============================] - 0s 831us/step - loss: 0.5161 - accuracy: 0.7708\n",
            "Epoch 196/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5269 - accuracy: 0.7465\n",
            "Epoch 197/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5062 - accuracy: 0.7500\n",
            "Epoch 198/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.7413\n",
            "Epoch 199/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5416 - accuracy: 0.7326\n",
            "Epoch 200/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5353 - accuracy: 0.7292\n",
            "Epoch 201/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4883 - accuracy: 0.7674\n",
            "Epoch 202/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.4923 - accuracy: 0.7448\n",
            "Epoch 203/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5044 - accuracy: 0.7413\n",
            "Epoch 204/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5231 - accuracy: 0.7309\n",
            "Epoch 205/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4880 - accuracy: 0.7604\n",
            "Epoch 206/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5160 - accuracy: 0.7361\n",
            "Epoch 207/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5046 - accuracy: 0.7569\n",
            "Epoch 208/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4999 - accuracy: 0.7552\n",
            "Epoch 209/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4924 - accuracy: 0.7674\n",
            "Epoch 210/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4836 - accuracy: 0.7726\n",
            "Epoch 211/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.4903 - accuracy: 0.7708\n",
            "Epoch 212/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4984 - accuracy: 0.7448\n",
            "Epoch 213/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5292 - accuracy: 0.7622\n",
            "Epoch 214/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5378 - accuracy: 0.7378\n",
            "Epoch 215/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5070 - accuracy: 0.7569\n",
            "Epoch 216/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.5471 - accuracy: 0.7257\n",
            "Epoch 217/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4956 - accuracy: 0.7535\n",
            "Epoch 218/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5022 - accuracy: 0.7691\n",
            "Epoch 219/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5397 - accuracy: 0.7535\n",
            "Epoch 220/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4922 - accuracy: 0.7569\n",
            "Epoch 221/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.7552\n",
            "Epoch 222/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7691\n",
            "Epoch 223/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4772 - accuracy: 0.7760\n",
            "Epoch 224/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5115 - accuracy: 0.7622\n",
            "Epoch 225/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5073 - accuracy: 0.7483\n",
            "Epoch 226/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.5305 - accuracy: 0.7396\n",
            "Epoch 227/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5160 - accuracy: 0.7448\n",
            "Epoch 228/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4746 - accuracy: 0.7726\n",
            "Epoch 229/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4788 - accuracy: 0.7691\n",
            "Epoch 230/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4894 - accuracy: 0.7500\n",
            "Epoch 231/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.5075 - accuracy: 0.7552\n",
            "Epoch 232/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4830 - accuracy: 0.7847\n",
            "Epoch 233/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4896 - accuracy: 0.7656\n",
            "Epoch 234/250\n",
            "18/18 [==============================] - 0s 765us/step - loss: 0.4757 - accuracy: 0.7674\n",
            "Epoch 235/250\n",
            "18/18 [==============================] - 0s 824us/step - loss: 0.4803 - accuracy: 0.7674\n",
            "Epoch 236/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5045 - accuracy: 0.7344\n",
            "Epoch 237/250\n",
            "18/18 [==============================] - 0s 823us/step - loss: 0.4886 - accuracy: 0.7778\n",
            "Epoch 238/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4798 - accuracy: 0.7674\n",
            "Epoch 239/250\n",
            "18/18 [==============================] - 0s 941us/step - loss: 0.5083 - accuracy: 0.7535\n",
            "Epoch 240/250\n",
            "18/18 [==============================] - 0s 941us/step - loss: 0.4717 - accuracy: 0.7847\n",
            "Epoch 241/250\n",
            "18/18 [==============================] - 0s 941us/step - loss: 0.4758 - accuracy: 0.7760\n",
            "Epoch 242/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.4855 - accuracy: 0.7674\n",
            "Epoch 243/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7656\n",
            "Epoch 244/250\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.5015 - accuracy: 0.7535\n",
            "Epoch 245/250\n",
            "18/18 [==============================] - 0s 941us/step - loss: 0.5200 - accuracy: 0.7378\n",
            "Epoch 246/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.5241 - accuracy: 0.7344\n",
            "Epoch 247/250\n",
            "18/18 [==============================] - 0s 1000us/step - loss: 0.4807 - accuracy: 0.7604\n",
            "Epoch 248/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4680 - accuracy: 0.7743\n",
            "Epoch 249/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4723 - accuracy: 0.7795\n",
            "Epoch 250/250\n",
            "18/18 [==============================] - 0s 882us/step - loss: 0.4794 - accuracy: 0.7587\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x23692737f50>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, activation = 'relu', input_dim = 8))\n",
        "model.add(Dense(16, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_train, y_train, epochs = 250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYIAZR2TMMSW"
      },
      "source": [
        "Puntuacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfQ9Tm5mMJ5N",
        "outputId": "16363004-4fca-4df3-f3ca-deedf5b28396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 882us/step - loss: 0.4665 - accuracy: 0.7847\n",
            "Precision Entrenamiento: 78.47%\n",
            "\n",
            "6/6 [==============================] - 0s 1000us/step - loss: 3.5386 - accuracy: 0.6719\n",
            "Precision Prueba: 67.19%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "scores = model.evaluate(X_train, y_train )\n",
        "print(\"Precision Entrenamiento: %.2f%%\\n\" % (scores[1] * 100))\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"Precision Prueba: %.2f%%\\n\" % (scores[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxx_QGodOhn5",
        "outputId": "e8fba532-564b-4b01-8ddb-1b37995c759e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 800us/step\n",
            "[0.   0.17 0.   0.69 0.   0.02 0.68 0.83 0.48 0.5  0.35 0.86 0.   0.42\n",
            " 0.   0.   0.86 0.   0.   0.52 0.   0.   0.   0.   0.09 0.01 0.   0.87\n",
            " 0.   0.   0.49 0.18 0.06 0.67 0.64 0.   0.6  0.03 0.87 1.   0.5  0.\n",
            " 0.02 0.   0.89 0.   0.31 0.48 0.47 0.   0.   0.61 0.84 0.02 0.2  0.47\n",
            " 0.   0.   0.   0.   0.31 0.38 0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.01 0.   0.   0.   0.   0.67 0.   0.03 0.   0.19 0.25 0.08\n",
            " 0.05 0.   0.   0.6  0.84 0.29 0.   0.   0.63 0.03 0.01 0.71 0.24 0.21\n",
            " 0.64 0.   0.19 0.37 0.   0.04 0.35 0.   0.06 0.01 0.01 0.   0.32 0.29\n",
            " 0.   0.06 0.33 0.   0.19 0.81 0.   0.7  0.01 0.   0.42 0.   0.71 0.\n",
            " 0.19 0.   0.   0.   0.54 0.   0.   0.77 0.01 0.01 0.   0.12 0.   0.\n",
            " 0.06 0.78 0.   0.   0.   0.   0.03 0.   0.   0.11 0.03 0.   0.16 0.18\n",
            " 0.5  0.   0.4  0.31 0.   0.   0.72 0.   0.2  0.49 0.   0.   0.   0.\n",
            " 0.   0.   0.49 0.   0.   0.   0.   0.13 0.   0.   0.17 0.   0.   0.\n",
            " 0.15 0.   0.   0.   0.   0.   0.65 0.17 0.   0.  ]\n",
            "[0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.88      0.78       130\n",
            "           1       0.48      0.24      0.32        62\n",
            "\n",
            "    accuracy                           0.67       192\n",
            "   macro avg       0.60      0.56      0.55       192\n",
            "weighted avg       0.64      0.67      0.63       192\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p_pred = model.predict(X_test)\n",
        "p_pred = p_pred.flatten()\n",
        "print(p_pred.round(2))\n",
        "\n",
        "\n",
        "\n",
        "y_pred = np.where(p_pred > 0.5, 1, 0)\n",
        "print(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crearemos una matriz de confusion para poder visualizar la cantidad de verdaderos positivos, falsos positivos, falsos negativos , falsos verdaderos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6YVjHtGAP4dd"
      },
      "outputs": [],
      "source": [
        "mapa_calor = confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "wzcPXOqIPlmb",
        "outputId": "39ef1d34-faa6-420b-eb11-edb6536c75b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sn.heatmap(mapa_calor, annot=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
